\documentclass[12pt]{article}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\title{Assignment 1}
\author{Rin Meng \\ Student ID: 51940633}

\begin{document}
\maketitle

\begin{enumerate}[1.]
    \item
    \begin{enumerate}[(a)]
        \item The linear regression model can be written as, 
        \[P = \beta_0 + \beta_1 C + \epsilon\]
        where,
        \begin{itemize}
            \item $P$ is the prime interest rate
            \item $C$ is the core inflation rate
            \item $\beta_0$ is the intercept of the regression line
            \item $\beta_1$ is the slope of the regression line
            \item $\epsilon$ is the error term
        \end{itemize}
        Assumptions of the linear regression model are,
        \begin{enumerate}[i.]
            \item \textbf{Linearity:} The relationship between $P$ and $C$ is linear
            \item \textbf{Independence:} The error term $\epsilon$ is independent of $C$
            \item \textbf{Normality:} The error term $\epsilon$ is normally distributed
            \item \textbf{Homoscedasticity:} The error term $\epsilon$ has a constant variance across all levels of $C$
        \end{enumerate}
    \end{enumerate}

    \item \textbf{Remark:} from ch2A.pdf slide 9 and 13,
        \begin{itemize}
            \item $\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}$
            \item $\hat{\beta_1} = \frac{S_{xy}}{S_{xx}}$
            \item $S_{xy} = \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})$
            \item $S_{xx} = \sum_{i=1}^{n} (x_i - \bar{x})^2$
        \end{itemize}
        Let $e_i$ be the $i$th residual term. Then for each observation, we can see that
            the residual for each observation $i$ is defined as: 
            \[e_i = y_i - \hat{y_i}\]
            then we can say that the predicted value $\hat{y_i}$ is equivalent to
            \[\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_i \]
    \begin{enumerate}[(a)]   
        \item $\sum_{i=1}^{n} e_i = 0$
            \begin{proof}
                LHS$\colon$
                \begin{align*}
                    \sum_{i=1}^{n} e_i &= \sum_{i=1}^{n} (y_i - \hat{y_i}) \\
                    &= \sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1} x_i) \\
                    &= \sum_{i=1}^{n}(y_i - \bar{y} + \hat{\beta_1}\bar{x} - \hat{\beta_1} x_i) \\
                    &= \sum_{i=1}^{n}y_i - n\bar{y} + \hat{\beta_1}n\bar{x} - \hat{\beta_1} \sum_{i=1}^{n}x_i \\
                    &= n\bar{y} - n\bar{y} + \hat{\beta_1}n\bar{x} - \hat{\beta_1}n\bar{x} \\ 
                    &= 0
                \end{align*}
                \[LHS = 0 = RHS\]
                $\therefore$ It is true that the sum of residuals $e_i$ is zero.
            \end{proof}
        \item $\sum_{i=1}^{n} x_i e_i = 0$
            \begin{proof}
                LHS$\colon$
                \begin{align*}
                    \sum_{i=1}^{n} x_i e_i &= \sum_{i=1}^{n} x_i (y_i - \hat{y_i}) \\
                    &= \sum_{i=1}^{n} x_i (y_i - \hat{\beta_0} - \hat{\beta_1} x_i) \\
                    &= \sum_{i=1}^{n} (x_i y_i - x_i (\bar{y} - \hat{\beta_1}\bar{x}) - \hat{\beta_1} x_i^2) \\
                    &= \sum_{i=1}^{n} (x_i y_i - x_i\bar{y} + x_i\hat{\beta_1}\bar{x} - \hat{\beta_1} x_i^2) \\
                    &= \sum_{i=1}^{n} (x_i(y_i - \bar{y}) + \hat{\beta_1}(\bar{x}x_i - x_i^2)) \\
                    &= \sum_{i=1}^n x_i y_i - \bar{y} \sum_{i=1}^n x_i - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - \bar{x} \sum_{i=1}^n x_i \right) \\
                    &= \left( \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - n \bar{x}^2 \right) \\
                    &= \left( \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} - n \bar{x} \bar{y} + n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 -  n \bar{x}\bar{x} -  n \bar{x}\bar{x} + n \bar{x}\bar{x} \right) \\
                    &= \left( \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} - n \bar{x} \bar{y} + n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - 2 n \bar{x}\bar{x} + n \bar{x}\bar{x} \right) \\
                    &= \left( \sum_{i=1}^n x_i y_i - \bar{y}\sum_{i=1}^n x_i - \bar{x}\sum_{i=1}^n y_i + n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - 2\bar{x}\sum_{i=1}^n x_i + n \bar{x}^2 \right) \\
                    &= \sum_{i=1}^n \left(  x_i y_i - \bar{y}x_i - \bar{x}y_i + \bar{x}\bar{y} \right) - \hat{\beta}_1 \sum_{i=1}^n \left( x_i^2 - 2\bar{x} x_i + \bar{x}^2 \right) \\
                    &= \sum_{i=1}^n \left(  x_i (y_i - \bar{y}) + \bar{x}(y_i - \bar{y}) \right) - \hat{\beta}_1 \sum_{i=1}^n (x_i - \bar{x})^2 \\
                    &= \sum_{i=1}^n (x_i + \bar{x})(y_i - \bar{y}) - \hat{\beta}_1 \sum_{i=1}^n (x_i - \bar{x})^2 \\
                    &= S_{xy} - \hat{\beta}_1 S_{xx} \\
                    &= S_{xy} - \frac{S_{xy}}{S_{xx}} S_{xx} \\
                    &= S_{xy} - S_{xy} \\
                    &= 0
                \end{align*}
                \[LHS = 0 = RHS\]
            \end{proof}
            $\therefore$ It is true that the independent variables $x_i$ is completly uncorrelated to the residuals $e_i$.
        \item $\sum_{i=1}^{n} \hat{y_i}e_i = 0$
            \begin{proof} 
                LHS$\colon$
                \begin{align*}
                    \sum_{i=1}^{n} \hat{y_i}e_i &= \sum_{i=1}^{n} \hat{y_i}(y_i - \hat{y_i}) \\
                    &= \sum_{i=1}^{n} (y_i\hat{y_i} - \hat{y_i}^2) \\
                    &= \sum_{i=1}^{n} (y_i(\hat{\beta_0} + \hat{\beta_1} x_i) - (\hat{\beta_0} + \hat{\beta_1} x_i)^2) \\
                    &= \sum_{i=1}^{n} (y_i(\bar{y} - \hat{\beta_1}\bar{x} + \hat{\beta_1} x_i) - (\bar{y} - \hat{\beta_1}\bar{x} + \hat{\beta_1} x_i)^2) \\
                    &= \sum_{i=1}^{n} (y_i\bar{y} - y_i\hat{\beta_1}\bar{x} + y_i\hat{\beta_1} x_i  \\
                    &- (\bar{y}^2 - 2\hat{\beta_1}\bar{y}\bar{x} + \hat{\beta_1}^2\bar{x}^2 + 2\hat{\beta_1}\bar{y}x_i - 2\hat{\beta_1}^2\bar{x}x_i + \hat{\beta_1}^2 x_i^2)) \\
                    &=  (\sum_{i=1}^{n}y_i\bar{y} - \sum_{i=1}^{n}y_i\hat{\beta_1}\bar{x} + \sum_{i=1}^{n}y_i\hat{\beta_1} x_i  \\
                    &- \sum_{i=1}^{n} (\bar{y}^2 - 2\hat{\beta_1}\bar{y}\bar{x} + \hat{\beta_1}^2\bar{x}^2 + 2\hat{\beta_1}\bar{y}x_i - 2\hat{\beta_1}^2\bar{x}x_i + \hat{\beta_1}^2 x_i^2)) \\
                    &= (n\bar{y}^2 - \hat{\beta_1}\bar{y}\bar{x} + \hat{\beta_1}\bar{y}\hat{\beta_1} x_i  \\
                    &- (\sum_{i=1}^{n} \bar{y}^2 - \sum_{i=1}^{n} 2\hat{\beta_1}\bar{y}\bar{x} + \sum_{i=1}^{n} \hat{\beta_1}^2\bar{x}^2 + \sum_{i=1}^{n}2\hat{\beta_1}\bar{y}x_i - \sum_{i=1}^{n}2\hat{\beta_1}^2\bar{x}x_i + \sum_{i=1}^{n}\hat{\beta_1}^2 x_i^2)) \\
                    &= (n\bar{y}^2 - (n\bar{y}^2 - 2\hat{\beta_1}n\bar{y}\bar{x} + \hat{\beta_1}^2n\bar{x}^2 + 2\hat{\beta_1}n\bar{y}\bar{x} - 2\hat{\beta_1}^2n\bar{x}\bar{x} + \hat{\beta_1}^2\sum_{i=1}^{n}x_i^2))
                \end{align*}
                
                proof continued.
                \begin{align*}
                    \sum_{i=1}^{n} \hat{y_i}e_i &= (n\bar{y}^2 - (n\bar{y}^2 - \hat{\beta_1}^2n\bar{x}^2 + \hat{\beta_1}^2n\bar{x}^2)) \\
                    &= (n\bar{y}^2 - n\bar{y}^2) \\
                    &= 0
                \end{align*}
                \[LHS = 0 = RHS\]   
            \end{proof}
            $\therefore$ It is true that the predicted values $\hat{y_i}$ is completely orthogonal to the residuals $e_i$.
    \end{enumerate}
    \item 
        \begin{enumerate}[(a)]
            \item It is given that:
            \begin{itemize}
                \item $\hat{\beta_1} = \frac{S_{xx}}{S_{xy}}$
                \item $S_{xy} = \sum_{i = 1}^{n} (x_i - \bar{x})(y_i - \bar{y})$
                \item $S_{xx} = \sum_{i = 1}^{n} (x_i - \bar{x})^2$
                \item $y_i = \hat{\beta_0} + \hat{\beta_1} x_i + \epsilon_i$
                \item $\bar{y} = \beta_0 + \beta_1 \bar{x}$
            \end{itemize}
            We want to show that 
            
            \[\hat{\beta_1} \sim N(\beta_1, \frac{\sigma^2}{S_{xx}})\]
            Where $\beta_1$ is the mean of the distribution and $\frac{\sigma^2}{S_{xx}}$ is the variance.
            \begin{proof}
            LHS$\colon$
            \begin{align*}
                \hat{\beta_1} &= \frac{S_{xy}}{S_{xx}} = \frac{\sum_{i = 1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{S_{xx}} \\
                &= \frac{\sum_{i = 1}^{n} (x_i - \bar{x})(\hat{\beta_0} + \hat{\beta_1} x_i + \epsilon_i - \beta_0 - \beta_1 \bar{x})}{S_{xx}} \\
            \end{align*}
            
            \end{proof}
        
    \end{enumerate}
    \item Given the model: 
        \[ y_i = \beta_1 + \beta_2 i + \epsilon_i \]
        \begin{enumerate}[(a)]
            \item We want to derive the least-squares estimators for $\beta_1$ and $\beta_2$.
            \begin{align*}
                S(\beta_1, \beta_2) &= \sum  (y_i - \beta_1 x_i - \beta_2 i)^2 \\
                \frac{\delta S}{\delta \beta_1} &= -2 \sum  (y_i - \beta_1 x_i - \beta_2 i)x_i = 0 \\
                &= -2 \sum  (y_i x_i - \beta_1 x_i^2 - \beta_2 i x_i)  = 0 \\
                \sum  y_i x_i &= \beta_1\sum  x_i^2 + \beta_2\sum  x_i i \\ 
                \frac{\delta S}{\delta \beta_2} &= -2 \sum  (y_i - \beta_1 x_i - \beta_2 i)i = 0 \\
                &= -2 \sum  (y_i i - \beta_1 x_i i - \beta_2 i^2) = 0 \\
                \sum  y_i i &= \beta_1\sum  x_i i + \beta_2\sum  i^2 
            \end{align*}
            Solving for $\beta_2$
            \begin{align*}
                \sum  y_i x_i &= \beta_1\sum  x_i^2 + \beta_2\sum  x_i i\\ 
                \beta_1\sum  x_i^2 &= \sum  y_i x_i - \beta_2\sum  x_i i \\ 
                \beta_1 &= \frac{\sum  y_i x_i - \beta_2\sum  x_i i}{\sum  x_i^2} \\ 
                \sum  y_i i &= \beta_1\sum  x_i i + \beta_2\sum  i^2 \\ 
                \beta_1\sum  x_i i &= \sum  y_i i - \beta_2\sum  i^2 \\ 
                \beta_1 &= \frac{ \sum  y_i i - \beta_2\sum  i^2}{\sum  x_i i}
            \end{align*}
            Now set them equal to each other and solve for $\beta_2$
            \begin{align*}
                \frac{\sum  y_i x_i - \beta_2\sum  x_i i}{\sum  x_i^2} &= \frac{ \sum  y_i i - \beta_2\sum  i^2}{\sum  x_i i} \\
                (\sum  y_i x_i - \beta_2\sum  x_i i) (\sum  x_i i) &= (\sum  x_i^2)(\sum  y_i i - \beta_2\sum  i^2) \\
                \sum  y_i x_i (\sum  x_i i) - \beta_2(\sum  x_i i)^2  &= \sum  y_i i (\sum  x_i^2) - \beta_2\sum  i^2 (\sum  x_i^2) \\
                \beta_2\sum  i^2 \sum  x_i^2 - \beta_2 (\sum  x_i i)^2 &= \sum  y_i i \sum  x_i^2 - \sum  y_i x_i \sum  x_i i \\
                \beta_2(\sum  i^2 \sum  x_i^2 - (\sum  x_i i)^2) &= \sum  y_i i \sum  x_i^2 - \sum  y_i x_i \sum  x_i i
            \end{align*}
            Finally, we have:
            \[ \beta_2 = \frac{\sum  y_i i \sum  x_i^2 - \sum  y_i x_i \sum  x_i i}{\sum  i^2 \sum  x_i^2 - (\sum  x_i i)^2}\]
            Solving for $\beta_1$ using same approach:
            \begin{align*}
                \sum  y_i x_i &= \beta_1\sum  x_i^2 + \beta_2\sum  x_i i \\ 
                \beta_2\sum  x_i i &= \sum  y_i x_i - \beta_1\sum  x_i^2 \\ 
                \beta_2 &= \frac{\sum  y_i x_i - \beta_1\sum  x_i^2}{\sum  x_i i} \\ 
                \sum  y_i i &= \beta_1\sum  x_i i + \beta_2\sum  i^2 \\ 
                \beta_2\sum  i^2 &= \sum  y_i i - \beta_1\sum  x_i i \\ 
                \beta_2 &= \frac{\sum  y_i i - \beta_1\sum  x_i i}{\sum  i^2}
            \end{align*}
            Now set them equal to each other and solve for $\beta_1$
            \begin{align*}
                \frac{\sum  y_i x_i - \beta_1\sum  x_i^2}{\sum  x_i i} &= \frac{\sum  y_i i - \beta_1\sum  x_i i}{\sum  i^2} \\
                (\sum  y_i x_i - \beta_1\sum  x_i^2)(\sum  i^2) &= (\sum  x_i i)(\sum  y_i i - \beta_1\sum  x_i i) \\
                \sum  y_i x_i \sum  i^2 - \beta_1\sum  x_i^2\sum  i^2 &= \sum  y_i i\sum  x_i i - \beta_1(\sum  x_i i)^2 \\
                \beta_1(\sum  x_i i)^2  - \beta_1\sum  x_i^2\sum  i^2 &= \sum  y_i i\sum  x_i i - \sum  y_i x_i \sum  i^2 \\
                \beta_1((\sum  x_i i)^2 - \sum  x_i^2\sum  i^2) &= \sum  y_i i\sum  x_i i - \sum  y_i x_i \sum  i^2 \\
            \end{align*}
            Finally, we have:
            \[ \beta_1 = \frac{\sum y_i i\sum x_i i - \sum y_i x_i \sum i^2}{(\sum x_i i)^2 - \sum x_i^2 \sum i^2 }\]

            To find the conditions where $x_i$ makes the estimators not well-defined, we let $x_i = i$.
            so then we have our $\beta_1$,
            \begin{align*}
                \beta_1 &= \frac{\sum y_i i\sum  x_i i - \sum y_i  x_i \sum i^2}{(\sum  x_i)^2 - \sum  x_i^2 \sum i^2 } \\
                &= \frac{\sum y_i i\sum  i^2 - \sum y_i  i \sum i^2}{(\sum i^2)^2 - \sum i^2 \sum i^2 }\\
                &= \frac{\sum y_i i\sum  i^2 - \sum y_i  i \sum i^2}{\sum i^2\sum i^2 - \sum i^2 \sum i^2 }\\
                &= \frac{0}{0}
            \end{align*}
            and then our $\beta_2$, 
            \begin{align*}
                \beta_2 &= \frac{\sum  y_i i \sum  x_i^2 - \sum  y_i x_i \sum  x_i i}{\sum  i^2 \sum  x_i^2 - (\sum  x_i i)^2} \\
                &= \frac{\sum  y_i i \sum   i^2 - \sum  y_i  i \sum   i^2}{(\sum i^2)^2 - \sum i^2 \sum i^2 }\\
                &= \frac{\sum  y_i i \sum   i^2 - \sum  y_i  i \sum   i^2}{\sum i^2 \sum i^2 - \sum i^2 \sum i^2 }\\
                &= \frac{0}{0}
            \end{align*}
            $\therefore$ The estimator $\beta_1$ and $\beta_2$ is not well-defined at $x_i = i$.

            \item For the case where the coefficient estimators are well-defined, the unbiased estimator for $\sigma^2$ is:
            \begin{align*}
                E[\sum \epsilon^2] &= (n-2)\sigma^2 \\ 
                \sigma^2 &= \frac{\sum \epsilon^2}{n-2}
            \end{align*}
            Derived from, Question 3b.
            
        \end{enumerate}
\end{enumerate}
\end{document}
