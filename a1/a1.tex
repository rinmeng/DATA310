\documentclass[12pt]{article}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\title{Assignment 1}
\author{Rin Meng \\ Student ID: 51940633}

\begin{document}
\maketitle

\begin{enumerate}[1.]
    \item
    \begin{enumerate}[a.]
        \item The linear regression model can be written as, 
        \[P = \beta_0 + \beta_1 C + \epsilon\]
        where,
        \begin{itemize}
            \item $P$ is the prime interest rate
            \item $C$ is the core inflation rate
            \item $\beta_0$ is the intercept of the regression line
            \item $\beta_1$ is the slope of the regression line
            \item $\epsilon$ is the error term
        \end{itemize}
        Assumptions of the linear regression model are,
        \begin{enumerate}[i.]
            \item \textbf{Linearity:} The relationship between $P$ and $C$ is linear
            \item \textbf{Independence:} The error term $\epsilon$ is independent of $C$
            \item \textbf{Normality:} The error term $\epsilon$ is normally distributed
            \item \textbf{Homoscedasticity:} The error term $\epsilon$ has a constant variance across all levels of $C$
        \end{enumerate}
    \end{enumerate}

    \item \textbf{Remark:} from ch2A.pdf slide 9 and 13,
        \begin{itemize}
            \item $\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}$
            \item $\hat{\beta_1} = \frac{S_{xy}}{S_{xx}}$
            \item $S_{xy} = \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})$
            \item $S_{xx} = \sum_{i=1}^{n} (x_i - \bar{x})^2$
        \end{itemize}
        Let $e_i$ be the $i$th residual term. Then for each observation, we can see that
            the residual for each observation $i$ is defined as: 
            \[e_i = y_i - \hat{y_i}\]
            then we can say that the predicted value $\hat{y_i}$ is equivalent to
            \[\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_i \]
    \begin{enumerate}[(a)]   
        \item $\sum_{i=1}^{n} e_i = 0$
            \begin{proof}
                LHS$\colon$
                \begin{align*}
                    \sum_{i=1}^{n} e_i &= \sum_{i=1}^{n} (y_i - \hat{y_i}) \\
                    &= \sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1} x_i) \\
                    &= \sum_{i=1}^{n}(y_i - \bar{y} + \hat{\beta_1}\bar{x} - \hat{\beta_1} x_i) \\
                    &= \sum_{i=1}^{n}y_i - n\bar{y} + \hat{\beta_1}n\bar{x} - \hat{\beta_1} \sum_{i=1}^{n}x_i \\
                    &= n\bar{y} - n\bar{y} + \hat{\beta_1}n\bar{x} - \hat{\beta_1}n\bar{x} \\ 
                    &= 0
                \end{align*}
                \[LHS = 0 = RHS\]
                $\therefore$ It is true that the sum of residuals $e_i$ is zero.
            \end{proof}
        \item $\sum_{i=1}^{n} x_i e_i = 0$
            \begin{proof}
                LHS$\colon$
                \begin{align*}
                    \sum_{i=1}^{n} x_i e_i &= \sum_{i=1}^{n} x_i (y_i - \hat{y_i}) \\
                    &= \sum_{i=1}^{n} x_i (y_i - \hat{\beta_0} - \hat{\beta_1} x_i) \\
                    &= \sum_{i=1}^{n} (x_i y_i - x_i (\bar{y} - \hat{\beta_1}\bar{x}) - \hat{\beta_1} x_i^2) \\
                    &= \sum_{i=1}^{n} (x_i y_i - x_i\bar{y} + x_i\hat{\beta_1}\bar{x} - \hat{\beta_1} x_i^2) \\
                    &= \sum_{i=1}^{n} (x_i(y_i - \bar{y}) + \hat{\beta_1}(\bar{x}x_i - x_i^2)) \\
                    &= \sum_{i=1}^n x_i y_i - \bar{y} \sum_{i=1}^n x_i - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - \bar{x} \sum_{i=1}^n x_i \right) \\
                    &= \left( \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - n \bar{x}^2 \right) \\
                    &= \left( \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} - n \bar{x} \bar{y} + n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 -  n \bar{x}\bar{x} -  n \bar{x}\bar{x} + n \bar{x}\bar{x} \right) \\
                    &= \left( \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} - n \bar{x} \bar{y} + n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - 2 n \bar{x}\bar{x} + n \bar{x}\bar{x} \right) \\
                    &= \left( \sum_{i=1}^n x_i y_i - \bar{y}\sum_{i=1}^n x_i - \bar{x}\sum_{i=1}^n y_i + n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - 2\bar{x}\sum_{i=1}^n x_i + n \bar{x}^2 \right) \\
                    &= \sum_{i=1}^n \left(  x_i y_i - \bar{y}x_i - \bar{x}y_i + \bar{x}\bar{y} \right) - \hat{\beta}_1 \sum_{i=1}^n \left( x_i^2 - 2\bar{x} x_i + \bar{x}^2 \right) \\
                    &= \sum_{i=1}^n \left(  x_i (y_i - \bar{y}) + \bar{x}(y_i - \bar{y}) \right) - \hat{\beta}_1 \sum_{i=1}^n (x_i - \bar{x})^2 \\
                    &= \sum_{i=1}^n (x_i + \bar{x})(y_i - \bar{y}) - \hat{\beta}_1 \sum_{i=1}^n (x_i - \bar{x})^2 \\
                    &= S_{xy} - \hat{\beta}_1 S_{xx} \\
                    &= S_{xy} - \frac{S_{xy}}{S_{xx}} S_{xx} \\
                    &= S_{xy} - S_{xy} \\
                    &= 0 \\
                \end{align*}
                \[LHS = 0 = RHS\]
            \end{proof}
            $\therefore$ It is true that the independent variables $x_i$ is completly uncorrelated to the residuals $e_i$.
        \item $\sum_{i=1}^{n} \hat{y_i}e_i = 0$
            \begin{proof} 
                LHS$\colon$
                \begin{align*}
                    \sum_{i=1}^{n} \hat{y_i}e_i &= \sum_{i=1}^{n} \hat{y_i}(y_i - \hat{y_i}) \\
                    &= \sum_{i=1}^{n} (y_i\hat{y_i} - \hat{y_i}^2) \\
                    &= \sum_{i=1}^{n} (y_i(\hat{\beta_0} + \hat{\beta_1} x_i) - (\hat{\beta_0} + \hat{\beta_1} x_i)^2) \\
                    &= \sum_{i=1}^{n} (y_i(\bar{y} - \hat{\beta_1}\bar{x} + \hat{\beta_1} x_i) - (\bar{y} - \hat{\beta_1}\bar{x} + \hat{\beta_1} x_i)^2) \\
                    &= \sum_{i=1}^{n} (y_i\bar{y} - y_i\hat{\beta_1}\bar{x} + y_i\hat{\beta_1} x_i  \\
                    &- (\bar{y}^2 - 2\hat{\beta_1}\bar{y}\bar{x} + \hat{\beta_1}^2\bar{x}^2 + 2\hat{\beta_1}\bar{y}x_i - 2\hat{\beta_1}^2\bar{x}x_i + \hat{\beta_1}^2 x_i^2)) \\
                    &=  (\sum_{i=1}^{n}y_i\bar{y} - \sum_{i=1}^{n}y_i\hat{\beta_1}\bar{x} + \sum_{i=1}^{n}y_i\hat{\beta_1} x_i  \\
                    &- \sum_{i=1}^{n} (\bar{y}^2 - 2\hat{\beta_1}\bar{y}\bar{x} + \hat{\beta_1}^2\bar{x}^2 + 2\hat{\beta_1}\bar{y}x_i - 2\hat{\beta_1}^2\bar{x}x_i + \hat{\beta_1}^2 x_i^2)) \\
                    &= (n\bar{y}^2 - \hat{\beta_1}\bar{y}\bar{x} + \hat{\beta_1}\bar{y}\hat{\beta_1} x_i  \\
                    &- (\sum_{i=1}^{n} \bar{y}^2 - \sum_{i=1}^{n} 2\hat{\beta_1}\bar{y}\bar{x} + \sum_{i=1}^{n} \hat{\beta_1}^2\bar{x}^2 + \sum_{i=1}^{n}2\hat{\beta_1}\bar{y}x_i - \sum_{i=1}^{n}2\hat{\beta_1}^2\bar{x}x_i + \sum_{i=1}^{n}\hat{\beta_1}^2 x_i^2)) \\
                    &= (n\bar{y}^2 - (n\bar{y}^2 - 2\hat{\beta_1}n\bar{y}\bar{x} + \hat{\beta_1}^2n\bar{x}^2 + 2\hat{\beta_1}n\bar{y}\bar{x} - 2\hat{\beta_1}^2n\bar{x}\bar{x} + \hat{\beta_1}^2\sum_{i=1}^{n}x_i^2)) \\
                \end{align*}
                
                proof continued.
                \begin{align*}
                    \sum_{i=1}^{n} \hat{y_i}e_i &= (n\bar{y}^2 - (n\bar{y}^2 - \hat{\beta_1}^2n\bar{x}^2 + \hat{\beta_1}^2n\bar{x}^2)) \\
                    &= (n\bar{y}^2 - n\bar{y}^2) \\
                    &= 0 \\
                \end{align*}
                \[LHS = 0 = RHS\]
            \end{proof}
            $\therefore$ It is true that the predicted values $\hat{y_i}$ is completely orthogonal to the residuals $e_i$.
    \end{enumerate}
    \item 
\end{enumerate}

\end{document}
