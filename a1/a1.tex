\documentclass[12pt]{article}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\title{Assignment 1}
\author{Rin Meng \\ Student ID: 51940633}

\begin{document}
\maketitle

\begin{enumerate}[1.]
    \item
    \begin{enumerate}[(a)]
        \item The linear regression model can be written as, 
        \[P = \beta_0 + \beta_1 C + \epsilon\]
        where,
        \begin{itemize}
            \item $P$ is the prime interest rate
            \item $C$ is the core inflation rate
            \item $\beta_0$ is the intercept of the regression line
            \item $\beta_1$ is the slope of the regression line
            \item $\epsilon$ is the error term
        \end{itemize}
        Assumptions of the linear regression model are,
        \begin{enumerate}[i.]
            \item \textbf{Linearity:} The relationship between $P$ and $C$ is linear
            \item \textbf{Independence:} The error term $\epsilon$ is independent of $C$
            \item \textbf{Normality:} The error term $\epsilon$ is normally distributed
            \item \textbf{Homoscedasticity:} The error term $\epsilon$ has a constant variance across all levels of $C$
        \end{enumerate}
    \end{enumerate}

    \item \textbf{Remark:} from ch2A.pdf slide 9 and 13,
        \begin{itemize}
            \item $\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}$
            \item $\hat{\beta_1} = \frac{S_{xy}}{S_{xx}}$
            \item $S_{xy} = \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})$
            \item $S_{xx} = \sum_{i=1}^{n} (x_i - \bar{x})^2$
        \end{itemize}
        Let $e_i$ be the $i$th residual term. Then for each observation, we can see that
            the residual for each observation $i$ is defined as: 
            \[e_i = y_i - \hat{y_i}\]
            then we can say that the predicted value $\hat{y_i}$ is equivalent to
            \[\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_i \]
    \begin{enumerate}[(a)]   
        \item $\sum_{i=1}^{n} e_i = 0$
            \begin{proof}
                LHS$\colon$
                \begin{align*}
                    \sum_{i=1}^{n} e_i &= \sum_{i=1}^{n} (y_i - \hat{y_i}) \\
                    &= \sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1} x_i) \\
                    &= \sum_{i=1}^{n}(y_i - \bar{y} + \hat{\beta_1}\bar{x} - \hat{\beta_1} x_i) \\
                    &= \sum_{i=1}^{n}y_i - n\bar{y} + \hat{\beta_1}n\bar{x} - \hat{\beta_1} \sum_{i=1}^{n}x_i \\
                    &= n\bar{y} - n\bar{y} + \hat{\beta_1}n\bar{x} - \hat{\beta_1}n\bar{x} \\ 
                    &= 0
                \end{align*}
                \[LHS = 0 = RHS\]
                $\therefore$ It is true that the sum of residuals $e_i$ is zero.
            \end{proof}
        \item $\sum_{i=1}^{n} x_i e_i = 0$
            \begin{proof}
                LHS$\colon$
                \begin{align*}
                    \sum_{i=1}^{n} x_i e_i &= \sum_{i=1}^{n} x_i (y_i - \hat{y_i}) \\
                    &= \sum_{i=1}^{n} x_i (y_i - \hat{\beta_0} - \hat{\beta_1} x_i) \\
                    &= \sum_{i=1}^{n} (x_i y_i - x_i (\bar{y} - \hat{\beta_1}\bar{x}) - \hat{\beta_1} x_i^2) \\
                    &= \sum_{i=1}^{n} (x_i y_i - x_i\bar{y} + x_i\hat{\beta_1}\bar{x} - \hat{\beta_1} x_i^2) \\
                    &= \sum_{i=1}^{n} (x_i(y_i - \bar{y}) + \hat{\beta_1}(\bar{x}x_i - x_i^2)) \\
                    &= \sum_{i=1}^n x_i y_i - \bar{y} \sum_{i=1}^n x_i - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - \bar{x} \sum_{i=1}^n x_i \right) \\
                    &= \left( \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - n \bar{x}^2 \right) \\
                    &= \left( \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} - n \bar{x} \bar{y} + n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 -  n \bar{x}\bar{x} -  n \bar{x}\bar{x} + n \bar{x}\bar{x} \right) \\
                    &= \left( \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} - n \bar{x} \bar{y} + n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - 2 n \bar{x}\bar{x} + n \bar{x}\bar{x} \right) \\
                    &= \left( \sum_{i=1}^n x_i y_i - \bar{y}\sum_{i=1}^n x_i - \bar{x}\sum_{i=1}^n y_i + n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - 2\bar{x}\sum_{i=1}^n x_i + n \bar{x}^2 \right) \\
                    &= \sum_{i=1}^n \left(  x_i y_i - \bar{y}x_i - \bar{x}y_i + \bar{x}\bar{y} \right) - \hat{\beta}_1 \sum_{i=1}^n \left( x_i^2 - 2\bar{x} x_i + \bar{x}^2 \right) \\
                    &= \sum_{i=1}^n \left(  x_i (y_i - \bar{y}) + \bar{x}(y_i - \bar{y}) \right) - \hat{\beta}_1 \sum_{i=1}^n (x_i - \bar{x})^2 \\
                    &= \sum_{i=1}^n (x_i + \bar{x})(y_i - \bar{y}) - \hat{\beta}_1 \sum_{i=1}^n (x_i - \bar{x})^2 \\
                    &= S_{xy} - \hat{\beta}_1 S_{xx} \\
                    &= S_{xy} - \frac{S_{xy}}{S_{xx}} S_{xx} \\
                    &= S_{xy} - S_{xy} \\
                    &= 0
                \end{align*}
                \[LHS = 0 = RHS\]
            \end{proof}
            $\therefore$ It is true that the independent variables $x_i$ is completly uncorrelated to the residuals $e_i$.
        \item $\sum_{i=1}^{n} \hat{y_i}e_i = 0$
            \begin{proof} 
                LHS$\colon$
                \begin{align*}
                    \sum_{i=1}^{n} \hat{y_i}e_i &= \sum_{i=1}^{n} \hat{y_i}(y_i - \hat{y_i}) \\
                    &= \sum_{i=1}^{n} (y_i\hat{y_i} - \hat{y_i}^2) \\
                    &= \sum_{i=1}^{n} (y_i(\hat{\beta_0} + \hat{\beta_1} x_i) - (\hat{\beta_0} + \hat{\beta_1} x_i)^2) \\
                    &= \sum_{i=1}^{n} (y_i(\bar{y} - \hat{\beta_1}\bar{x} + \hat{\beta_1} x_i) - (\bar{y} - \hat{\beta_1}\bar{x} + \hat{\beta_1} x_i)^2) \\
                    &= \sum_{i=1}^{n} (y_i\bar{y} - y_i\hat{\beta_1}\bar{x} + y_i\hat{\beta_1} x_i  \\
                    &- (\bar{y}^2 - 2\hat{\beta_1}\bar{y}\bar{x} + \hat{\beta_1}^2\bar{x}^2 + 2\hat{\beta_1}\bar{y}x_i - 2\hat{\beta_1}^2\bar{x}x_i + \hat{\beta_1}^2 x_i^2)) \\
                    &=  (\sum_{i=1}^{n}y_i\bar{y} - \sum_{i=1}^{n}y_i\hat{\beta_1}\bar{x} + \sum_{i=1}^{n}y_i\hat{\beta_1} x_i  \\
                    &- \sum_{i=1}^{n} (\bar{y}^2 - 2\hat{\beta_1}\bar{y}\bar{x} + \hat{\beta_1}^2\bar{x}^2 + 2\hat{\beta_1}\bar{y}x_i - 2\hat{\beta_1}^2\bar{x}x_i + \hat{\beta_1}^2 x_i^2)) \\
                    &= (n\bar{y}^2 - \hat{\beta_1}\bar{y}\bar{x} + \hat{\beta_1}\bar{y}\hat{\beta_1} x_i  \\
                    &- (\sum_{i=1}^{n} \bar{y}^2 - \sum_{i=1}^{n} 2\hat{\beta_1}\bar{y}\bar{x} + \sum_{i=1}^{n} \hat{\beta_1}^2\bar{x}^2 + \sum_{i=1}^{n}2\hat{\beta_1}\bar{y}x_i - \sum_{i=1}^{n}2\hat{\beta_1}^2\bar{x}x_i + \sum_{i=1}^{n}\hat{\beta_1}^2 x_i^2)) \\
                    &= (n\bar{y}^2 - (n\bar{y}^2 - 2\hat{\beta_1}n\bar{y}\bar{x} + \hat{\beta_1}^2n\bar{x}^2 + 2\hat{\beta_1}n\bar{y}\bar{x} - 2\hat{\beta_1}^2n\bar{x}\bar{x} + \hat{\beta_1}^2\sum_{i=1}^{n}x_i^2))
                \end{align*}
                
                proof continued.
                \begin{align*}
                    \sum_{i=1}^{n} \hat{y_i}e_i &= (n\bar{y}^2 - (n\bar{y}^2 - \hat{\beta_1}^2n\bar{x}^2 + \hat{\beta_1}^2n\bar{x}^2)) \\
                    &= (n\bar{y}^2 - n\bar{y}^2) \\
                    &= 0
                \end{align*}
                \[LHS = 0 = RHS\]   
            \end{proof}
            $\therefore$ It is true that the predicted values $\hat{y_i}$ is completely orthogonal to the residuals $e_i$.
    \end{enumerate}
    \item 
        \begin{enumerate}[(a)]
            \item It is given that:
            \begin{itemize}
                \item $\hat{\beta_1} = \frac{S_{xx}}{S_{xy}}$
                \item $S_{xy} = \sum_{i = 1}^{n} (x_i - \bar{x})(y_i - \bar{y})$
                \item $S_{xx} = \sum_{i = 1}^{n} (x_i - \bar{x})^2$
                \item $y_i = \hat{\beta_0} + \hat{\beta_1} x_i + \epsilon_i$
                \item $\bar{y} = \beta_0 + \beta_1 \bar{x}$
            \end{itemize}
            We want to show that 
            \[\hat{\beta_1} \sim N(\beta_1, \frac{\sigma^2}{S_{xx}})\]
            Where $\beta_1$ is the mean of the distribution and $\frac{\sigma^2}{S_{xx}}$ is the variance.
            \begin{proof}
            Let $k_i = \frac{x_i - \bar{x}}{S_{xx}}$ then we have
            \[ \hat{\beta_1} = \sum k_i (y_i - \bar{y})\]
            \[ E[\hat{\beta_1}] = E\left[ \sum k_i (y_i - \bar{y}) \right] \]
            \[=  \sum k_i E[(y_i - \bar{y})] \]
            \[=  \sum k_i E[(\hat{\beta_0} + \hat{\beta_1} x_i + \epsilon_i - \beta_0 - \beta_1 \bar{x})] \]
            \[=  \sum k_i (E[\hat{\beta_0}] + E[\hat{\beta_1} x_i] + E[\epsilon_i] - E[\beta_0] - E[\beta_1 \bar{x}]) \]
            \[=  \sum k_i (0 + \beta_1 x_i + 0 - 0 - \beta_1 \bar{x}) \]
            \[=  \sum k_i (\beta_1 x_i - \beta_1 \bar{x}) \]
            \[=  \beta_1 \sum k_i (x_i - \bar{x}) \]
            \[=  \beta_1 \sum \frac{x_i - \bar{x}}{S_{xx}} (x_i - \bar{x}) \]
            \[=  \beta_1 \sum \frac{(x_i - \bar{x})^2}{(x_i - \bar{x})^2}  \]
            \[=  \beta_1 \]
            $\therefore E[\hat{\beta_1}] = \beta_1 $
            \end{proof}
            Using $k_i$ from earlier, we can proof that $Var[\hat{\beta_1}] = \frac{\sigma^2}{S_{xx}}$ 
            \[ Var[\hat{\beta_1}] = Var[\sum k_i (y_i - \bar{y})] \]
            \[ Var[\hat{\beta_1}] = \sum Var[k_i (y_i - \bar{y})] \]
            \[ Var[\hat{\beta_1}] = \sum k_i^2 Var[(y_i - \bar{y})] \]
    \end{enumerate}
    \item Given the model: 
        \[ y_i = \beta_1 + \beta_2 i + \epsilon_i \]
        \begin{enumerate}[(a)]
            \item Let us derive the least-squares estimators for $\beta_1$ and $\beta_2$.
            \begin{align*}
                S(\beta_1, \beta_2) &= \sum  (y_i - \beta_1 x_i - \beta_2 i)^2 \\
                \frac{\delta S}{\delta \beta_1} &= -2 \sum  (y_i - \beta_1 x_i - \beta_2 i)x_i = 0 \\
                &= -2 \sum  (y_i x_i - \beta_1 x_i^2 - \beta_2 i x_i)  = 0 \\
                \sum  y_i x_i &= \beta_1\sum  x_i^2 + \beta_2\sum  x_i i \\ 
                \frac{\delta S}{\delta \beta_2} &= -2 \sum  (y_i - \beta_1 x_i - \beta_2 i)i = 0 \\
                &= -2 \sum  (y_i i - \beta_1 x_i i - \beta_2 i^2) = 0 \\
                \sum  y_i i &= \beta_1\sum  x_i i + \beta_2\sum  i^2 
            \end{align*}
            Solving for $\beta_2$
            \begin{align*}
                \sum  y_i x_i &= \beta_1\sum  x_i^2 + \beta_2\sum  x_i i\\ 
                \beta_1\sum  x_i^2 &= \sum  y_i x_i - \beta_2\sum  x_i i \\ 
                \beta_1 &= \frac{\sum  y_i x_i - \beta_2\sum  x_i i}{\sum  x_i^2} \\ 
                \sum  y_i i &= \beta_1\sum  x_i i + \beta_2\sum  i^2 \\ 
                \beta_1\sum  x_i i &= \sum  y_i i - \beta_2\sum  i^2 \\ 
                \beta_1 &= \frac{ \sum  y_i i - \beta_2\sum  i^2}{\sum  x_i i}
            \end{align*}
            Now set them equal to each other and solve for $\beta_2$
            \begin{align*}
                \frac{\sum  y_i x_i - \beta_2\sum  x_i i}{\sum  x_i^2} &= \frac{ \sum  y_i i - \beta_2\sum  i^2}{\sum  x_i i} \\
                (\sum  y_i x_i - \beta_2\sum  x_i i) (\sum  x_i i) &= (\sum  x_i^2)(\sum  y_i i - \beta_2\sum  i^2) \\
                \sum  y_i x_i (\sum  x_i i) - \beta_2(\sum  x_i i)^2  &= \sum  y_i i (\sum  x_i^2) - \beta_2\sum  i^2 (\sum  x_i^2) \\
                \beta_2\sum  i^2 \sum  x_i^2 - \beta_2 (\sum  x_i i)^2 &= \sum  y_i i \sum  x_i^2 - \sum  y_i x_i \sum  x_i i \\
                \beta_2(\sum  i^2 \sum  x_i^2 - (\sum  x_i i)^2) &= \sum  y_i i \sum  x_i^2 - \sum  y_i x_i \sum  x_i i
            \end{align*}
            Finally, we have:
            \[ \beta_2 = \frac{\sum  y_i i \sum  x_i^2 - \sum  y_i x_i \sum  x_i i}{\sum  i^2 \sum  x_i^2 - (\sum  x_i i)^2}\]
            Solving for $\beta_1$ using same approach:
            \begin{align*}
                \sum  y_i x_i &= \beta_1\sum  x_i^2 + \beta_2\sum  x_i i \\ 
                \beta_2\sum  x_i i &= \sum  y_i x_i - \beta_1\sum  x_i^2 \\ 
                \beta_2 &= \frac{\sum  y_i x_i - \beta_1\sum  x_i^2}{\sum  x_i i} \\ 
                \sum  y_i i &= \beta_1\sum  x_i i + \beta_2\sum  i^2 \\ 
                \beta_2\sum  i^2 &= \sum  y_i i - \beta_1\sum  x_i i \\ 
                \beta_2 &= \frac{\sum  y_i i - \beta_1\sum  x_i i}{\sum  i^2}
            \end{align*}
            Now set them equal to each other and solve for $\beta_1$
            \begin{align*}
                \frac{\sum  y_i x_i - \beta_1\sum  x_i^2}{\sum  x_i i} &= \frac{\sum  y_i i - \beta_1\sum  x_i i}{\sum  i^2} \\
                (\sum  y_i x_i - \beta_1\sum  x_i^2)(\sum  i^2) &= (\sum  x_i i)(\sum  y_i i - \beta_1\sum  x_i i) \\
                \sum  y_i x_i \sum  i^2 - \beta_1\sum  x_i^2\sum  i^2 &= \sum  y_i i\sum  x_i i - \beta_1(\sum  x_i i)^2 \\
                \beta_1(\sum  x_i i)^2  - \beta_1\sum  x_i^2\sum  i^2 &= \sum  y_i i\sum  x_i i - \sum  y_i x_i \sum  i^2 \\
                \beta_1((\sum  x_i i)^2 - \sum  x_i^2\sum  i^2) &= \sum  y_i i\sum  x_i i - \sum  y_i x_i \sum  i^2 \\
            \end{align*}
            Finally, we have:
            \[ \beta_1 = \frac{\sum y_i i\sum x_i i - \sum y_i x_i \sum i^2}{(\sum x_i i)^2 - \sum x_i^2 \sum i^2 }\]

            To find the conditions where $x_i$ makes the estimators not well-defined, we let $x_i = i$.
            so then we have our $\beta_1$,
            \begin{align*}
                \beta_1 &= \frac{\sum y_i i\sum  x_i i - \sum y_i  x_i \sum i^2}{(\sum  x_i)^2 - \sum  x_i^2 \sum i^2 } \\
                &= \frac{\sum y_i i\sum  i^2 - \sum y_i  i \sum i^2}{(\sum i^2)^2 - \sum i^2 \sum i^2 }\\
                &= \frac{\sum y_i i\sum  i^2 - \sum y_i  i \sum i^2}{\sum i^2\sum i^2 - \sum i^2 \sum i^2 }\\
                &= \frac{0}{0}
            \end{align*}
            and then our $\beta_2$, 
            \begin{align*}
                \beta_2 &= \frac{\sum  y_i i \sum  x_i^2 - \sum  y_i x_i \sum  x_i i}{\sum  i^2 \sum  x_i^2 - (\sum  x_i i)^2} \\
                &= \frac{\sum  y_i i \sum   i^2 - \sum  y_i  i \sum   i^2}{(\sum i^2)^2 - \sum i^2 \sum i^2 }\\
                &= \frac{\sum  y_i i \sum   i^2 - \sum  y_i  i \sum   i^2}{\sum i^2 \sum i^2 - \sum i^2 \sum i^2 }\\
                &= \frac{0}{0}
            \end{align*}
            $\therefore$ The estimator $\beta_1$ and $\beta_2$ is not well-defined at $x_i = i$.

            \item For the case where the coefficient estimators are well-defined, the unbiased estimator for $\sigma^2$ is:
            \begin{align*}
                E[\sum \epsilon^2] &= (n-2)\sigma^2 \\ 
                \sigma^2 &= \frac{\sum \epsilon^2}{n-2}
            \end{align*}
            Derived from, Question 3b.
        \end{enumerate}[(a)]
    
        \item 
        \begin{proof} We want to show that $MSE$ is an unbiased estimator $\sigma^2$ then we want to show that:
            \[E[MSE] = E\left[\frac{1}{n-1} \sum (y_i - \bar{y})^2 \right] = \sigma^2 \]
            so then we can start by recalling the model,
            \[ y_i = \alpha + \epsilon_i \]
            then it is true that the sample mean is,
            \[ \bar{y} = \alpha + \bar{\epsilon}\]
            then, $y_i - \bar{y}$ can be rewritten as,
            \[ y_i - \bar{y} = (\alpha + \epsilon_i) - (\alpha + \bar{\epsilon}) = \epsilon_i - \bar{\epsilon}\]
            Now, we have
            \begin{align*}
                E[MSE] &= E\left[\frac{1}{n-1} \sum (\epsilon_i - \bar{\epsilon})^2 \right] \\ 
                &= \frac{1}{n-1} E\left[(\sum \epsilon_i^2 - \sum \bar{\epsilon}^2) \right] \\ 
                &= \frac{1}{n-1} (\sum E[\epsilon_i^2] - E[n\bar{\epsilon}^2] )
            \end{align*}
            We know that 
            \[E[\epsilon_i] = 0 \Rightarrow E[\epsilon_i^2] = Var[\epsilon_i] + E[\epsilon_i]^2 = \sigma^2\]
            \[\sum E[\epsilon_i] = n\sigma^2\]
            now we find that,
            \begin{align*}
                E[n\bar{\epsilon}^2] &= nE[\bar{\epsilon}^2] \\
                &\Rightarrow E[\bar{\epsilon}^2] = Var[\bar{\epsilon}] + E[\bar{\epsilon}]^2 \\
                &= \frac{\sigma^2}{n}\\
                &\Leftrightarrow Var[\bar{\epsilon}] = Var[\frac{1}{n} \sum \epsilon_i] \\
                &= \frac{1}{n^2} \sum Var[\epsilon_i] \\
                &= \frac{1}{n^2} \sum \sigma^2 \\
                &= \frac{1}{n^2} n \sigma^2 = \frac{\sigma^2}{n}  \\
                &\Leftrightarrow E[\bar{\epsilon}]^2 = E[\frac{1}{n} \sum e_i]^2 \\ 
                &= (\frac{1}{n} \sum E[e_i])^2 \\ 
                &= (\frac{1}{n} \cdot 0)^2  = 0 \\ 
                E[n\bar{\epsilon}^2] &= n \cdot \frac{\sigma^2}{n} = \sigma^2
            \end{align*}
            so then finally we have:
            \[E[MSE] = \frac{1}{n-1}(n\sigma^2 - \sigma^2) \]
            \[= \frac{1}{n-1} (n-1) \sigma^2 \]
            \[=  \sigma^2\]
            $\therefore$ $MSE$ is an unbiased estimator for $\sigma^2$.
        \end{proof}

        \item 
        \begin{enumerate}[(a)]
            \item The linear regression model can be written as,
            \[ R = \beta_0 + \beta_1 W + \epsilon\]
            Where,
            \begin{itemize}
                \item $R$ is the rate of the spread of a wildfire m/s
                \item $W$ is the wind speed km/h
                \item $\beta_0$ is the the rate of spread when $W = 0$
                \item $\beta_1$ is the slope the rate of spread increases with one-unit wind speed
                \item $\epsilon$ is the error term
            \end{itemize}
            \item Assumptions that is made on the error term $\epsilon$ are:
            \begin{enumerate}[i.]
            \item \textbf{Independence:} The error term $\epsilon$ is independent of $W$
            \item $E[\epsilon] = 0$
            \item $Var[\epsilon] = \sigma^2$
            \end{enumerate}
            \item The expected value of $R$ given that $W = x$ is:
            \[ E[R|W = x] = \beta_0 + \beta_1 x \Leftrightarrow E[\epsilon] = 0\]
            \item The variance of $R$ given that $W = x$ is:
            \[ Var[R|W = x] = Var(\beta_0 + \beta_1 x + \epsilon|x) = \sigma^2 \]
        \end{enumerate}
            
            
\end{enumerate}
\end{document}
