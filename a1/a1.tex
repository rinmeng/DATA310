\documentclass[12pt]{article}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{changepage}

\title{Assignment 1}
\author{Rin Meng \\ Student ID: 51940633}

\begin{document}
\maketitle

\begin{enumerate}[1.]
    \item
    \begin{enumerate}[(a)]
        \item The linear regression model can be written as, 
        \[P = \beta_0 + \beta_1 C + \epsilon\]
        where,
        \begin{itemize}
            \item $P$ is the prime interest rate
            \item $C$ is the core inflation rate
            \item $\beta_0$ is the intercept of the regression line
            \item $\beta_1$ is the slope of the regression line
            \item $\epsilon$ is the error term
        \end{itemize}
        Assumptions of the linear regression model are,
        \begin{enumerate}[i.]
            \item \textbf{Linearity:} The relationship between $P$ and $C$ is linear
            \item \textbf{Independence:} The error term $\epsilon$ is independent of $C$
            \item \textbf{Normality:} The error term $\epsilon$ is normally distributed
            \item \textbf{Homoscedasticity:} The error term $\epsilon$ has a constant variance across all levels of $C$
        \end{enumerate}
    \end{enumerate}

    \item \textbf{Remark:} from ch2A.pdf slide 9 and 13,
        \begin{itemize}
            \item $\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}$
            \item $\hat{\beta_1} = \frac{S_{xy}}{S_{xx}}$
            \item $S_{xy} = \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})$
            \item $S_{xx} = \sum_{i=1}^{n} (x_i - \bar{x})^2$
        \end{itemize}
        Let $e_i$ be the $i$th residual term. Then for each observation, we can see that
            the residual for each observation $i$ is defined as: 
            \[e_i = y_i - \hat{y_i}\]
            then we can say that the predicted value $\hat{y_i}$ is equivalent to
            \[\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_i \]
    \begin{enumerate}[(a)]   
        \item $\sum_{i=1}^{n} e_i = 0$
            \begin{proof}
                LHS$\colon$
                \begin{align*}
                    \sum_{i=1}^{n} e_i &= \sum_{i=1}^{n} (y_i - \hat{y_i}) \\
                    &= \sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1} x_i) \\
                    &= \sum_{i=1}^{n}(y_i - \bar{y} + \hat{\beta_1}\bar{x} - \hat{\beta_1} x_i) \\
                    &= \sum_{i=1}^{n}y_i - n\bar{y} + \hat{\beta_1}n\bar{x} - \hat{\beta_1} \sum_{i=1}^{n}x_i \\
                    &= n\bar{y} - n\bar{y} + \hat{\beta_1}n\bar{x} - \hat{\beta_1}n\bar{x} \\ 
                    &= 0
                \end{align*}
                \[LHS = 0 = RHS\]
                $\therefore$ It is true that the sum of residuals $e_i$ is zero.
            \end{proof}
        \item $\sum_{i=1}^{n} x_i e_i = 0$
            \begin{proof}
                LHS$\colon$
                \begin{align*}
                    \sum_{i=1}^{n} x_i e_i &= \sum_{i=1}^{n} x_i (y_i - \hat{y_i}) \\
                    &= \sum_{i=1}^{n} x_i (y_i - \hat{\beta_0} - \hat{\beta_1} x_i) \\
                    &= \sum_{i=1}^{n} (x_i y_i - x_i (\bar{y} - \hat{\beta_1}\bar{x}) - \hat{\beta_1} x_i^2) \\
                    &= \sum_{i=1}^{n} (x_i y_i - x_i\bar{y} + x_i\hat{\beta_1}\bar{x} - \hat{\beta_1} x_i^2) \\
                    &= \sum_{i=1}^{n} (x_i(y_i - \bar{y}) + \hat{\beta_1}(\bar{x}x_i - x_i^2)) \\
                    &= \sum_{i=1}^n x_i y_i - \bar{y} \sum_{i=1}^n x_i - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - \bar{x} \sum_{i=1}^n x_i \right) \\
                    &= \left( \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - n \bar{x}^2 \right) \\
                    &= \left( \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} - n \bar{x} \bar{y} + n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 -  n \bar{x}\bar{x} -  n \bar{x}\bar{x} + n \bar{x}\bar{x} \right) \\
                    &= \left( \sum_{i=1}^n x_i y_i - n \bar{x} \bar{y} - n \bar{x} \bar{y} + n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - 2 n \bar{x}\bar{x} + n \bar{x}\bar{x} \right) \\
                    &= \left( \sum_{i=1}^n x_i y_i - \bar{y}\sum_{i=1}^n x_i - \bar{x}\sum_{i=1}^n y_i + n \bar{x} \bar{y} \right) - \hat{\beta}_1 \left( \sum_{i=1}^n x_i^2 - 2\bar{x}\sum_{i=1}^n x_i + n \bar{x}^2 \right) \\
                    &= \sum_{i=1}^n \left(  x_i y_i - \bar{y}x_i - \bar{x}y_i + \bar{x}\bar{y} \right) - \hat{\beta}_1 \sum_{i=1}^n \left( x_i^2 - 2\bar{x} x_i + \bar{x}^2 \right) \\
                    &= \sum_{i=1}^n \left(  x_i (y_i - \bar{y}) + \bar{x}(y_i - \bar{y}) \right) - \hat{\beta}_1 \sum_{i=1}^n (x_i - \bar{x})^2 \\
                    &= \sum_{i=1}^n (x_i + \bar{x})(y_i - \bar{y}) - \hat{\beta}_1 \sum_{i=1}^n (x_i - \bar{x})^2 \\
                    &= S_{xy} - \hat{\beta}_1 S_{xx} \\
                    &= S_{xy} - \frac{S_{xy}}{S_{xx}} S_{xx} \\
                    &= S_{xy} - S_{xy} \\
                    &= 0
                \end{align*}
                \[LHS = 0 = RHS\]
            \end{proof}
            $\therefore$ It is true that the independent variables $x_i$ is completly uncorrelated to the residuals $e_i$.
        \item $\sum_{i=1}^{n} \hat{y_i}e_i = 0$
            \begin{proof} 
                LHS$\colon$
                \begin{align*}
                    \sum_{i=1}^{n} \hat{y_i}e_i &= \sum_{i=1}^{n} \hat{y_i}(y_i - \hat{y_i}) \\
                    &= \sum_{i=1}^{n} (y_i\hat{y_i} - \hat{y_i}^2) \\
                    &= \sum_{i=1}^{n} (y_i(\hat{\beta_0} + \hat{\beta_1} x_i) - (\hat{\beta_0} + \hat{\beta_1} x_i)^2) \\
                    &= \sum_{i=1}^{n} (y_i(\bar{y} - \hat{\beta_1}\bar{x} + \hat{\beta_1} x_i) - (\bar{y} - \hat{\beta_1}\bar{x} + \hat{\beta_1} x_i)^2) \\
                    &= \sum_{i=1}^{n} (y_i\bar{y} - y_i\hat{\beta_1}\bar{x} + y_i\hat{\beta_1} x_i  \\
                    &- (\bar{y}^2 - 2\hat{\beta_1}\bar{y}\bar{x} + \hat{\beta_1}^2\bar{x}^2 + 2\hat{\beta_1}\bar{y}x_i - 2\hat{\beta_1}^2\bar{x}x_i + \hat{\beta_1}^2 x_i^2)) \\
                    &=  (\sum_{i=1}^{n}y_i\bar{y} - \sum_{i=1}^{n}y_i\hat{\beta_1}\bar{x} + \sum_{i=1}^{n}y_i\hat{\beta_1} x_i  \\
                    &- \sum_{i=1}^{n} (\bar{y}^2 - 2\hat{\beta_1}\bar{y}\bar{x} + \hat{\beta_1}^2\bar{x}^2 + 2\hat{\beta_1}\bar{y}x_i - 2\hat{\beta_1}^2\bar{x}x_i + \hat{\beta_1}^2 x_i^2)) \\
                    &= (n\bar{y}^2 - \hat{\beta_1}\bar{y}\bar{x} + \hat{\beta_1}\bar{y}\hat{\beta_1} x_i  \\
                    &- (\sum_{i=1}^{n} \bar{y}^2 - \sum_{i=1}^{n} 2\hat{\beta_1}\bar{y}\bar{x} + \sum_{i=1}^{n} \hat{\beta_1}^2\bar{x}^2 + \sum_{i=1}^{n}2\hat{\beta_1}\bar{y}x_i - \sum_{i=1}^{n}2\hat{\beta_1}^2\bar{x}x_i + \sum_{i=1}^{n}\hat{\beta_1}^2 x_i^2)) \\
                    &= (n\bar{y}^2 - (n\bar{y}^2 - 2\hat{\beta_1}n\bar{y}\bar{x} + \hat{\beta_1}^2n\bar{x}^2 + 2\hat{\beta_1}n\bar{y}\bar{x} - 2\hat{\beta_1}^2n\bar{x}\bar{x} + \hat{\beta_1}^2\sum_{i=1}^{n}x_i^2))
                \end{align*}
                
                proof continued.
                \begin{align*}
                    \sum_{i=1}^{n} \hat{y_i}e_i &= (n\bar{y}^2 - (n\bar{y}^2 - \hat{\beta_1}^2n\bar{x}^2 + \hat{\beta_1}^2n\bar{x}^2)) \\
                    &= (n\bar{y}^2 - n\bar{y}^2) \\
                    &= 0
                \end{align*}
                \[LHS = 0 = RHS\]   
            \end{proof}
            $\therefore$ It is true that the predicted values $\hat{y_i}$ is completely orthogonal to the residuals $e_i$.
    \end{enumerate}
    \item 
        \begin{enumerate}[(a)]
            \item It is given that:
            \begin{itemize}
                \item $\hat{\beta_1} = \frac{S_{xx}}{S_{xy}}$
                \item $S_{xy} = \sum_{i = 1}^{n} (x_i - \bar{x})(y_i - \bar{y})$
                \item $S_{xx} = \sum_{i = 1}^{n} (x_i - \bar{x})^2$
                \item $y_i = \hat{\beta_0} + \hat{\beta_1} x_i + \epsilon_i$
                \item $\bar{y} = \beta_0 + \beta_1 \bar{x}$
            \end{itemize}
            We want to show that 
            \[\hat{\beta_1} \sim N(\beta_1, \frac{\sigma^2}{S_{xx}})\]
            Where $\beta_1$ is the mean of the distribution and $\frac{\sigma^2}{S_{xx}}$ is the variance.
            \begin{proof}
            Let $k_i = \frac{x_i - \bar{x}}{S_{xx}}$ then we have
            \[ \hat{\beta_1} = \sum k_i (y_i - \bar{y})\]
            \[ E[\hat{\beta_1}] = E\left[ \sum k_i (y_i - \bar{y}) \right] \]
            \[=  \sum k_i E[(y_i - \bar{y})] \]
            \[=  \sum k_i E[(\hat{\beta_0} + \hat{\beta_1} x_i + \epsilon_i - \beta_0 - \beta_1 \bar{x})] \]
            \[=  \sum k_i (E[\hat{\beta_0}] + E[\hat{\beta_1} x_i] + E[\epsilon_i] - E[\beta_0] - E[\beta_1 \bar{x}]) \]
            \[=  \sum k_i (0 + \beta_1 x_i + 0 - 0 - \beta_1 \bar{x}) \]
            \[=  \sum k_i (\beta_1 x_i - \beta_1 \bar{x}) \]
            \[=  \beta_1 \sum k_i (x_i - \bar{x}) \]
            \[=  \beta_1 \sum \frac{x_i - \bar{x}}{S_{xx}} (x_i - \bar{x}) \]
            \[=  \beta_1 \sum \frac{(x_i - \bar{x})^2}{(x_i - \bar{x})^2}  \]
            \[=  \beta_1 \]
            $\therefore E[\hat{\beta_1}] = \beta_1 $
            \end{proof}
            Using $k_i$ from earlier, we can proof that $Var[\hat{\beta_1}] = \frac{\sigma^2}{S_{xx}}$ 
            \[ Var[\hat{\beta_1}] = Var[\sum k_i (y_i - \bar{y})] \]
            \[ Var[\hat{\beta_1}] = \sum Var[k_i (y_i - \bar{y})] \]
            \[ Var[\hat{\beta_1}] = \sum k_i^2 Var[(y_i - \bar{y})] \]
    \end{enumerate}
    \item Given the model: 
        \[ y_i = \beta_1 + \beta_2 i + \epsilon_i \]
        \begin{enumerate}[(a)]
            \item Let us derive the least-squares estimators for $\beta_1$ and $\beta_2$.
            \begin{align*}
                S(\beta_1, \beta_2) &= \sum  (y_i - \beta_1 x_i - \beta_2 i)^2 \\
                \frac{\delta S}{\delta \beta_1} &= -2 \sum  (y_i - \beta_1 x_i - \beta_2 i)x_i = 0 \\
                &= -2 \sum  (y_i x_i - \beta_1 x_i^2 - \beta_2 i x_i)  = 0 \\
                \sum  y_i x_i &= \beta_1\sum  x_i^2 + \beta_2\sum  x_i i \\ 
                \frac{\delta S}{\delta \beta_2} &= -2 \sum  (y_i - \beta_1 x_i - \beta_2 i)i = 0 \\
                &= -2 \sum  (y_i i - \beta_1 x_i i - \beta_2 i^2) = 0 \\
                \sum  y_i i &= \beta_1\sum  x_i i + \beta_2\sum  i^2 
            \end{align*}
            Solving for $\beta_2$
            \begin{align*}
                \sum  y_i x_i &= \beta_1\sum  x_i^2 + \beta_2\sum  x_i i\\ 
                \beta_1\sum  x_i^2 &= \sum  y_i x_i - \beta_2\sum  x_i i \\ 
                \beta_1 &= \frac{\sum  y_i x_i - \beta_2\sum  x_i i}{\sum  x_i^2} \\ 
                \sum  y_i i &= \beta_1\sum  x_i i + \beta_2\sum  i^2 \\ 
                \beta_1\sum  x_i i &= \sum  y_i i - \beta_2\sum  i^2 \\ 
                \beta_1 &= \frac{ \sum  y_i i - \beta_2\sum  i^2}{\sum  x_i i}
            \end{align*}
            Now set them equal to each other and solve for $\beta_2$
            \begin{align*}
                \frac{\sum  y_i x_i - \beta_2\sum  x_i i}{\sum  x_i^2} &= \frac{ \sum  y_i i - \beta_2\sum  i^2}{\sum  x_i i} \\
                (\sum  y_i x_i - \beta_2\sum  x_i i) (\sum  x_i i) &= (\sum  x_i^2)(\sum  y_i i - \beta_2\sum  i^2) \\
                \sum  y_i x_i (\sum  x_i i) - \beta_2(\sum  x_i i)^2  &= \sum  y_i i (\sum  x_i^2) - \beta_2\sum  i^2 (\sum  x_i^2) \\
                \beta_2\sum  i^2 \sum  x_i^2 - \beta_2 (\sum  x_i i)^2 &= \sum  y_i i \sum  x_i^2 - \sum  y_i x_i \sum  x_i i \\
                \beta_2(\sum  i^2 \sum  x_i^2 - (\sum  x_i i)^2) &= \sum  y_i i \sum  x_i^2 - \sum  y_i x_i \sum  x_i i
            \end{align*}
            Finally, we have:
            \[ \beta_2 = \frac{\sum  y_i i \sum  x_i^2 - \sum  y_i x_i \sum  x_i i}{\sum  i^2 \sum  x_i^2 - (\sum  x_i i)^2}\]
            Solving for $\beta_1$ using same approach:
            \begin{align*}
                \sum  y_i x_i &= \beta_1\sum  x_i^2 + \beta_2\sum  x_i i \\ 
                \beta_2\sum  x_i i &= \sum  y_i x_i - \beta_1\sum  x_i^2 \\ 
                \beta_2 &= \frac{\sum  y_i x_i - \beta_1\sum  x_i^2}{\sum  x_i i} \\ 
                \sum  y_i i &= \beta_1\sum  x_i i + \beta_2\sum  i^2 \\ 
                \beta_2\sum  i^2 &= \sum  y_i i - \beta_1\sum  x_i i \\ 
                \beta_2 &= \frac{\sum  y_i i - \beta_1\sum  x_i i}{\sum  i^2}
            \end{align*}
            Now set them equal to each other and solve for $\beta_1$
            \begin{align*}
                \frac{\sum  y_i x_i - \beta_1\sum  x_i^2}{\sum  x_i i} &= \frac{\sum  y_i i - \beta_1\sum  x_i i}{\sum  i^2} \\
                (\sum  y_i x_i - \beta_1\sum  x_i^2)(\sum  i^2) &= (\sum  x_i i)(\sum  y_i i - \beta_1\sum  x_i i) \\
                \sum  y_i x_i \sum  i^2 - \beta_1\sum  x_i^2\sum  i^2 &= \sum  y_i i\sum  x_i i - \beta_1(\sum  x_i i)^2 \\
                \beta_1(\sum  x_i i)^2  - \beta_1\sum  x_i^2\sum  i^2 &= \sum  y_i i\sum  x_i i - \sum  y_i x_i \sum  i^2 \\
                \beta_1((\sum  x_i i)^2 - \sum  x_i^2\sum  i^2) &= \sum  y_i i\sum  x_i i - \sum  y_i x_i \sum  i^2 \\
            \end{align*}
            Finally, we have:
            \[ \beta_1 = \frac{\sum y_i i\sum x_i i - \sum y_i x_i \sum i^2}{(\sum x_i i)^2 - \sum x_i^2 \sum i^2 }\]

            To find the conditions where $x_i$ makes the estimators not well-defined, we let $x_i = i$.
            so then we have our $\beta_1$,
            \begin{align*}
                \beta_1 &= \frac{\sum y_i i\sum  x_i i - \sum y_i  x_i \sum i^2}{(\sum  x_i)^2 - \sum  x_i^2 \sum i^2 } \\
                &= \frac{\sum y_i i\sum  i^2 - \sum y_i  i \sum i^2}{(\sum i^2)^2 - \sum i^2 \sum i^2 }\\
                &= \frac{\sum y_i i\sum  i^2 - \sum y_i  i \sum i^2}{\sum i^2\sum i^2 - \sum i^2 \sum i^2 }\\
                &= \frac{0}{0}
            \end{align*}
            and then our $\beta_2$, 
            \begin{align*}
                \beta_2 &= \frac{\sum  y_i i \sum  x_i^2 - \sum  y_i x_i \sum  x_i i}{\sum  i^2 \sum  x_i^2 - (\sum  x_i i)^2} \\
                &= \frac{\sum  y_i i \sum   i^2 - \sum  y_i  i \sum   i^2}{(\sum i^2)^2 - \sum i^2 \sum i^2 }\\
                &= \frac{\sum  y_i i \sum   i^2 - \sum  y_i  i \sum   i^2}{\sum i^2 \sum i^2 - \sum i^2 \sum i^2 }\\
                &= \frac{0}{0}
            \end{align*}
            $\therefore$ The estimator $\beta_1$ and $\beta_2$ is not well-defined at $x_i = i$.

            \item For the case where the coefficient estimators are well-defined, the unbiased estimator for $\sigma^2$ is:
            \begin{align*}
                E[\sum \epsilon^2] &= (n-2)\sigma^2 \\ 
                \sigma^2 &= \frac{\sum \epsilon^2}{n-2}
            \end{align*}
            Derived from, Question 3b.
        \end{enumerate}[(a)]
    
        \item 
        \begin{proof} We want to show that $MSE$ is an unbiased estimator $\sigma^2$ then we want to show that:
            \[E[MSE] = E\left[\frac{1}{n-1} \sum (y_i - \bar{y})^2 \right] = \sigma^2 \]
            so then we can start by recalling the model,
            \[ y_i = \alpha + \epsilon_i \]
            then it is true that the sample mean is,
            \[ \bar{y} = \alpha + \bar{\epsilon}\]
            then, $y_i - \bar{y}$ can be rewritten as,
            \[ y_i - \bar{y} = (\alpha + \epsilon_i) - (\alpha + \bar{\epsilon}) = \epsilon_i - \bar{\epsilon}\]
            Now, we have
            \begin{align*}
                E[MSE] &= E\left[\frac{1}{n-1} \sum (\epsilon_i - \bar{\epsilon})^2 \right] \\ 
                &= \frac{1}{n-1} E\left[(\sum \epsilon_i^2 - \sum \bar{\epsilon}^2) \right] \\ 
                &= \frac{1}{n-1} (\sum E[\epsilon_i^2] - E[n\bar{\epsilon}^2] )
            \end{align*}
            We know that 
            \[E[\epsilon_i] = 0 \Rightarrow E[\epsilon_i^2] = Var[\epsilon_i] + E[\epsilon_i]^2 = \sigma^2\]
            \[\sum E[\epsilon_i] = n\sigma^2\]
            now we find that,
            \begin{align*}
                E[n\bar{\epsilon}^2] &= nE[\bar{\epsilon}^2] \\
                &\Rightarrow E[\bar{\epsilon}^2] = Var[\bar{\epsilon}] + E[\bar{\epsilon}]^2 \\
                &= \frac{\sigma^2}{n}\\
                &\Leftrightarrow Var[\bar{\epsilon}] = Var[\frac{1}{n} \sum \epsilon_i] \\
                &= \frac{1}{n^2} \sum Var[\epsilon_i] \\
                &= \frac{1}{n^2} \sum \sigma^2 \\
                &= \frac{1}{n^2} n \sigma^2 = \frac{\sigma^2}{n}  \\
                &\Leftrightarrow E[\bar{\epsilon}]^2 = E[\frac{1}{n} \sum e_i]^2 \\ 
                &= (\frac{1}{n} \sum E[e_i])^2 \\ 
                &= (\frac{1}{n} \cdot 0)^2  = 0 \\ 
                E[n\bar{\epsilon}^2] &= n \cdot \frac{\sigma^2}{n} = \sigma^2
            \end{align*}
            so then finally we have:
            \[E[MSE] = \frac{1}{n-1}(n\sigma^2 - \sigma^2) \]
            \[= \frac{1}{n-1} (n-1) \sigma^2 \]
            \[=  \sigma^2\]
            $\therefore$ $MSE$ is an unbiased estimator for $\sigma^2$.
        \end{proof}

        \item 
        \begin{enumerate}[(a)]
            \item The linear regression model can be written as,
            \[ R = \beta_0 + \beta_1 W + \epsilon\]
            Where,
            \begin{itemize}
                \item $R$ is the rate of the spread of a wildfire m/s
                \item $W$ is the wind speed km/h
                \item $\beta_0$ is the the rate of spread when $W = 0$
                \item $\beta_1$ is the slope the rate of spread increases with one-unit wind speed
                \item $\epsilon$ is the error term
            \end{itemize}
            \item Assumptions that is made on the error term $\epsilon$ are:
            \begin{enumerate}[i.]
            \item \textbf{Independence:} The error term $\epsilon$ is independent of $W$
            \item $E[\epsilon] = 0$
            \item $Var[\epsilon] = \sigma^2$
            \end{enumerate}
            \item The expected value of $R$ given that $W = x$ is:
            \[ E[R|W = x] = \beta_0 + \beta_1 x \Leftrightarrow E[\epsilon] = 0\]
            \item The variance of $R$ given that $W = x$ is:
            \[ Var[R|W = x] = Var(\beta_0 + \beta_1 x + \epsilon|x) = \sigma^2 \]
            \item Given that W is a random variable with mean $\mu_W$ and variance $\sigma_W^2$,
            \begin{enumerate}[i.]
                \item The unconditional expected value of $R$ is:
                \[ E[R] =  E[\beta_0 + \beta_1 W + \epsilon] = \beta_0 + \beta_1 E[W] + E[\epsilon] = \beta_0 + \beta_1 \mu_W \]
                Here, $\mu_W$ is the mean of the wind speed. This contrasts with part (c) because in (c) we are looking for the expected value of
                $R$ given $W = x$, and here we are looking for expected value of $R$ given that $W$ is a random variable.
                \item The unconditional variance of $R$ is:
                \[ Var[R] = Var[\beta_0 + \beta_1 W + \epsilon] = Var[\beta_0] + \beta_1^2 Var[W] + Var[\epsilon] \]
                \[= \beta_1^2 \sigma_W^2 + \sigma^2 \]
                This contrasts with part (d) because in (d) we are looking for the specific condition where speed 
                 is fixed at $x$ and which equates to $\sigma^2$. Now, we are including all of the variability of $W$ instead of fixing it to a particular value.
            \end{enumerate}
            \item If the rate of spread of a wildfire $W$ is linearly related to the square root of wind speed $\sqrt{W}$,
            then the model will intuitively be:
            \[ R = \beta_0 + \beta_1 \sqrt{W} + \epsilon\]
            
        \end{enumerate}
        \item 
        From Question 6, we have, \
        \[R : \{ 30, 32, 18, 35, 12 \} \]
        \[W : \{ 35, 40, 20, 50, 15 \} \]
        \begin{enumerate}[(a)]
            \item 
            \begin{align*}
                \hat{\beta_1} &= \frac{S_{xy}}{S_{xx}} = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \\
                &= \frac{\sum_{i=1}^{n} (W_i - \bar{W})(R_i - \bar{R})}{\sum_{i=1}^{n} (W_i - \bar{W})^2} \\
                \bar{R} &= \frac{30 + 32 + 18 + 35 + 12}{5} = 25.4 \\
                \bar{W} &= \frac{35 + 40 + 20 + 50 + 15}{5} = 32
            \end{align*}
            Calculating $S_{xy}$:
            \begin{center}
                \begin{tabular}{|c|c|c|c|c|}
                    \hline
                    $W_i$ & $R_i$ & $W_i - \bar{W}$ & $R_i - \bar{R}$ & $(W_i - \bar{W})(R_i - \bar{R})$\\
                    \hline
                    35 & 30 & $35 - 32 = 3$ & $30 - 25.4 = 4.6$ & $3 \cdot 4.6 = 13.8$ \\
                    \hline
                    40 & 32 & $40 - 32 = 8$ & $32 - 25.4 = 6.6$ & $8 \cdot 6.6 = 52.8$ \\
                    \hline
                    20 & 18 & $20 - 32 = -12$ & $18 - 25.4 = -7.4$ & $-12 \cdot -7.4 = 88.8$ \\
                    \hline
                    50 & 35 & $50 - 32 = 18$ & $35 - 25.4 = 9.6$ & $18 \cdot 9.6 = 172.8$ \\
                    \hline
                    15 & 12 & $15 - 32 = -17$ & $12 - 25.4 = -13.4$ & $-17 \cdot -13.4 = 227.8$ \\
                    \hline
                \end{tabular}
                \[ S_{xy} = 13.8 + 52.8 + 88.8 + 172.8 + 227.8 = 556 \]
            \end{center}
            Calculating $S_{xx}$:
            \begin{center} 
                \begin{tabular}{|c|c|c|c|}
                    \hline
                    $W_i$ & $W_i - \bar{W}$ & $(W_i - \bar{W})^2$\\
                    \hline
                    35 & $35 - 32 = 3$ & $3^2 = 9$ \\
                    \hline
                    40 & $40 - 32 = 8$ & $8^2 = 64$ \\
                    \hline
                    20 & $20 - 32 = -12$ & $(-12)^2 = 144$ \\
                    \hline
                    50 & $50 - 32 = 18$ & $18^2 = 324$ \\
                    \hline
                    15 & $15 - 32 = -17$ & $(-17)^2 = 289$ \\
                    \hline
                \end{tabular}
                \[ S_{xx} = 9 + 64 + 144 + 324 + 289 = 830 \]
            \end{center}
            Finally, we have:
            \[ \hat{\beta_1} = \frac{556}{830} = 0.6707 \]
            \item Recall from (a) that $\bar{R} = 25.4$ and $\bar{W} = 32$. We can now calculate $\hat{\beta_0}$:
            \begin{align*}
                \hat{\beta_0} &= \bar{R} - \hat{\beta_1} \bar{W} \\
                &= 25.4 - 0.6707 \cdot 32 \\
                &= 25.4 - 21.47 \\
                &= 3.9376
            \end{align*}
            \item 
            \[ \hat{\sigma}^2 =  \frac{1}{n-2} \sum (y_i - \hat{y_i})^2 = \frac{1}{5-2} \sum (R_i - \hat{R_i})^2 \]
            With the values, our model is now:
            \[ \hat{R} = 3.9376 + 0.6707W \]
            \begin{center}
                \begin{tabular}{|c|c|c|c|c|c|}
                    \hline
                    $W_i$ & $R_i$ & $\hat{R_i}$ & $R_i - \hat{R_i}$ & $(R_i - \hat{R_i})^2$\\
                    \hline
                    35 & 30 & 27.4121 & 2.5879 & 6.6972 \\
                    \hline
                    40 & 32 & 30.7656 & 1.2344 & 1.5237 \\
                    \hline
                    20 & 18 & 17.3516 & 0.6484 & 0.4204 \\
                    \hline
                    50 & 35 & 37.4726 & -2.4726 & 6.1137 \\
                    \hline
                    15 & 12 & 13.9981 & -1.9981 & 3.9924 \\
                    \hline
                \end{tabular}
                \[ \hat{\sigma}^2 = \frac{6.6972 + 1.5237 + 0.4204 + 6.1137 + 3.9924}{3} = \frac{18.7474}{6.2491} \simeq 6.25 \]
            \end{center}
            \item
            \begin{verbatim}
                R <- c(30, 32, 18, 35, 12)
                W <- c(35, 40, 20, 50, 15)
                speedwind <- data.frame(R, W)
            \end{verbatim}
            \item Code: 
            \begin{verbatim}
                model <- lm(R ~ W, data = speedwind)
                summary(model)
            \end{verbatim}
            
            \begin{center}
                Summary returns:
            \end{center}
            \begin{adjustwidth}{-5cm}{}
                \begin{verbatim}
                    Call:
                    lm(formula = R ~ W, data = speedwind)
                    
                    Residuals:
                        1       2       3       4       5 
                    2.5904  1.2410  0.6386 -2.4578 -2.0120 
                    
                    Coefficients:
                                Estimate Std. Error t value Pr(>|t|)   
                    (Intercept)  3.96386    2.99323   1.324  0.27726   
                    W            0.66988    0.08677   7.720  0.00452 **
                    ---
                    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
                    
                    Residual standard error: 2.5 on 3 degrees of freedom
                    Multiple R-squared:  0.9521,	Adjusted R-squared:  0.9361 
                    F-statistic:  59.6 on 1 and 3 DF,  p-value: 0.004518
                \end{verbatim}
            \end{adjustwidth}
                Our $\beta_0$ value is 3.93766 and $\beta_1$ value is 3.96386 which is relatively close, 
                and our $\beta_1$ value is 0.6707 which is also relatively close to the value we calculated at 0.66988.
                our residual standard error, $\sigma$ is also on point at $\sigma = \sqrt{\sigma^2} = \sqrt{6.25} = 2.5$ compared to the output.
                Overall, our calculations are not far off from the output of the linear regression model in R.
        \end{enumerate}
\end{enumerate}
End of Assignment 1.
\end{document}
