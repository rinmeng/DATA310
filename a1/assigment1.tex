\documentclass[12pt]{article}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

\title{Assignment 1}
\author{Rin Meng \\ Student ID: 51940633}

\begin{document}
\maketitle

\begin{enumerate}[1.]
    \item
    \begin{enumerate}[a.]
        \item The linear regression model can be written as, 
        \[P = \beta_0 + \beta_1 C + \epsilon\]
        where,
        \begin{itemize}
            \item $P$ is the prime interest rate
            \item $C$ is the core inflation rate
            \item $\beta_0$ is the intercept of the regression line
            \item $\beta_1$ is the slope of the regression line
            \item $\epsilon$ is the error term
        \end{itemize}
        Assumptions of the linear regression model are,
        \begin{enumerate}[i.]
            \item \textbf{Linearity:} The relationship between $P$ and $C$ is linear
            \item \textbf{Independence:} The error term $\epsilon$ is independent of $C$
            \item \textbf{Normality:} The error term $\epsilon$ is normally distributed
            \item \textbf{Homoscedasticity:} The error term $\epsilon$ has a constant variance across all levels of $C$
        \end{enumerate}
    \end{enumerate}

    \item If the simple linear regression model can be written as $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$,
        where $E[\epsilon_i] = 0$ and $E[\epsilon^2_i] = \sigma^2$ and where the $\epsilon$'s are independent,
        the ith fitten value is denoted by $\hat{y_i}$ and the $i$th residual is denoted by 
        $e_i$ $(i = 1, 2 , \ldots , n)$, we can show that: 
    \begin{enumerate}[(a)]   
        \item $\sum_{i=1}^{n} e_i = 0$
        \begin{proof}
            Let $e_i$ be the $i$th residual term. Then for each observation, we can see that
            the residual for each observation $i$ is defined as: 
            \[e_i = y_i - \hat{y_i}\]
            where,
            \begin{itemize}
                \item $y_i$ is the actual observed value of the dependent variable.
                \item $\hat{y_i}$ is the predicted value obtained from the regression model.
            \end{itemize}
            then we can say that the predicted value $\hat{y_i}$ is equivalent to
            \[\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_i \]
            Where,
            \begin{itemize}
                \item $\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x}$
                \item $\hat{\beta_1} = \frac{S_{xy}}{S_{xx}}$
                \item $S_{xy} = \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})$
                \item $S_{xx} = \sum_{i=1}^{n} (x_i - \bar{x})^2$
            \end{itemize}
            We want to show that:
            \[\sum_{i=1}^{n} e_i = 0\]
            LHS$\colon$
            \begin{align*}
                \sum_{i=1}^{n} e_i &= \sum_{i=1}^{n} (y_i - \hat{y_i}) \\
                &= \sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1} x_i) \\
                &= \sum_{i=1}^{n}(y_i - \bar{y} + \hat{\beta_1}\bar{x} - \hat{\beta_1} x_i) \\
                &= \sum_{i=1}^{n}y_i - n\bar{y} + \hat{\beta_1}n\bar{x} - \hat{\beta_1} \sum_{i=1}^{n}x_i \\
                &= n\bar{y} - n\bar{y} + \hat{\beta_1}n\bar{x} - \hat{\beta_1}n\bar{x} \\ 
                &= 0
            \end{align*}
            \[LHS = 0 = RHS\]
            $\therefore$ The sum of residuals $e_i$ is zero.

            
            
            

        \end{proof}
    \end{enumerate}
\end{enumerate}

\end{document}
